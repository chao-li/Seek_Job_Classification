{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "# import\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_science = []\n",
    "for i in range(1, 501):\n",
    "    \n",
    "    path = 'https://www.seek.com.au/Data-Science-jobs/in-All-Australia?page=' + str(i)\n",
    "    path_data_science.append(path)\n",
    "    \n",
    "for i in range(1, 501):\n",
    "    \n",
    "    path = 'https://www.seek.com.au/Data-Scientist-jobs/in-All-Australia?page=' + str(i)\n",
    "    path_data_science.append(path)\n",
    "    \n",
    "for i in range(1, 501):\n",
    "    \n",
    "    path = 'https://www.seek.com.au/Machine-Learning-jobs/in-All-Australia?page=' + str(i)\n",
    "    path_data_science.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9246348ef924c9f848e55c9e42812af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1500)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates 480\n"
     ]
    }
   ],
   "source": [
    "urls_data_science = []\n",
    "dupe_count = 0\n",
    "for path in log_progress(path_data_science):\n",
    "    # load the page\n",
    "    page = requests.get(path)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # grab title url\n",
    "    titles = soup.find_all('a', {'data-automation': 'jobTitle'}) \n",
    "    for title in titles:\n",
    "        url = 'https://www.seek.com.au' + title.get('href')\n",
    "        if url not in urls_data_science:\n",
    "            urls_data_science.append(url)\n",
    "        else:\n",
    "            dupe_count+=1\n",
    "\n",
    "print('duplicates', dupe_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls_data_science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_data_science_df = pd.DataFrame(urls_data_science, columns = ['urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_data_science_df.to_csv('./Q2_data_scraping/seek_urls_data_science.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape all Data Science jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_data_science_df = pd.read_csv('./Q2_data_scraping/seek_urls_data_science.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_data_science = urls_data_science_df.urls.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbeb756de0ee476283c7069c7e27e60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=858)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "title_list = []\n",
    "company_list = []\n",
    "date_list = []\n",
    "location_list = []\n",
    "work_type_list = []\n",
    "job_type_list = []\n",
    "description_list = []\n",
    "url_list = []\n",
    "\n",
    "for url in log_progress(urls_data_science):\n",
    "    detailed_path = url\n",
    "    page = requests.get(detailed_path)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    ## URL\n",
    "    url_list.append(url)\n",
    "    \n",
    "    ## TITLES\n",
    "    title_object = soup.find('span', {'data-automation': 'job-detail-title'})\n",
    "    title = None\n",
    "    try:\n",
    "        title = title_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    title_list.append(title)\n",
    "    \n",
    "    ## COMPANY\n",
    "    company = None\n",
    "\n",
    "    try:\n",
    "        company_object = soup.find('span', {'data-automation': 'advertiser-name'})\n",
    "        company = company_object.text\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    company_list.append(company)\n",
    "    \n",
    "    \n",
    "    ## DATE\n",
    "    date_object = soup.find('dd', {'data-automation': 'job-detail-date'})\n",
    "    date = None\n",
    "\n",
    "    try:\n",
    "        date = date_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    date_list.append(date)\n",
    "    \n",
    "    ## LOCATION\n",
    "    location = None\n",
    "    try:\n",
    "        info_header = soup.find('section', {'aria-labelledby': 'jobInfoHeader'})\n",
    "        location_object = info_header.find('strong', {'class': 'lwHBT6d'})\n",
    "        location= location_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    location_list.append(location)\n",
    "    \n",
    "    ## WORK TYPE\n",
    "    work_type = None\n",
    "\n",
    "    try:\n",
    "        work_type_object = soup.find('dd',{'data-automation': 'job-detail-work-type'})\n",
    "        work_type = work_type_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    work_type_list.append(work_type)\n",
    "    \n",
    "    ## JOB TYPE\n",
    "    job_type = None\n",
    "\n",
    "    try:\n",
    "        info_header = soup.find('section', {'aria-labelledby': 'jobInfoHeader'})\n",
    "        job_type = info_header.find_all('dd')[-1].find('strong', {'class': 'lwHBT6d'}).text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    job_type_list.append(job_type)\n",
    "    \n",
    "    ## DESCRIPTION\n",
    "    description = None\n",
    "\n",
    "    try:\n",
    "        description_object = soup.find('div', {'data-automation': 'jobDescription'})\n",
    "        for p in description_object.find_all('p'):\n",
    "            if description == None:\n",
    "                description = p.text\n",
    "            else: \n",
    "                description = description + ' ' + p.text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        description_object = soup.find('div', {'data-automation': 'jobDescription'})\n",
    "        for li in description_object.find_all('li'):\n",
    "            if description == None:\n",
    "                description = li.text\n",
    "            else: \n",
    "                description = description + ' ' + li.textb\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    description_list.append(description)\n",
    "    \n",
    "\n",
    "seek_data_science_df = pd.DataFrame({'url': url_list,\n",
    "                               'job_title': title_list,\n",
    "                               'company': company_list,\n",
    "                               'date': date_list,\n",
    "                               'location': location_list,\n",
    "                               'employment_type': work_type_list,\n",
    "                               'field': job_type_list,\n",
    "                               'description': description_list})\n",
    "\n",
    "seek_data_science_df.to_csv('./Q2_data_scraping/seek_data_science.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seek_data_science_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analytic jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_analytic = []\n",
    "for i in range(1, 1001):\n",
    "    path = 'https://www.seek.com.au/Data-Analytic-jobs/in-All-Australia?page=' + str(i)\n",
    "    path_data_analytic.append(path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9d53b206694efab49c21ff6d10cbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates 413\n"
     ]
    }
   ],
   "source": [
    "urls_data_analytic = []\n",
    "dupe_count = 0\n",
    "for path in log_progress(path_data_analytic):\n",
    "    # load the page\n",
    "    page = requests.get(path)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # grab title url\n",
    "    titles = soup.find_all('a', {'data-automation': 'jobTitle'}) \n",
    "    for title in titles:\n",
    "        url = 'https://www.seek.com.au' + title.get('href')\n",
    "        if url not in urls_data_science and url not in urls_data_analytic:\n",
    "            urls_data_analytic.append(url)\n",
    "        else:\n",
    "            dupe_count+=1\n",
    "\n",
    "print('duplicates', dupe_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls_data_analytic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_data_analytic_df = pd.DataFrame(urls_data_analytic, columns = ['urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_data_analytic_df.to_csv('./Q2_data_scraping/seek_urls_data_analytic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape all data analytic jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_data_analytic_df = pd.read_csv('./Q2_data_scraping/seek_urls_data_analytic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_data_analytic = urls_data_analytic_df.urls.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0efea2a937a41c5a84b63fa19778f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1379)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "title_list = []\n",
    "company_list = []\n",
    "date_list = []\n",
    "location_list = []\n",
    "work_type_list = []\n",
    "job_type_list = []\n",
    "description_list = []\n",
    "url_list = []\n",
    "\n",
    "for url in log_progress(urls_data_analytic):\n",
    "    detailed_path = url\n",
    "    page = requests.get(detailed_path)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    ## URL\n",
    "    url_list.append(url)\n",
    "    \n",
    "    ## TITLES\n",
    "    title_object = soup.find('span', {'data-automation': 'job-detail-title'})\n",
    "    title = None\n",
    "    try:\n",
    "        title = title_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    title_list.append(title)\n",
    "    \n",
    "    ## COMPANY\n",
    "    company = None\n",
    "\n",
    "    try:\n",
    "        company_object = soup.find('span', {'data-automation': 'advertiser-name'})\n",
    "        company = company_object.text\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    company_list.append(company)\n",
    "    \n",
    "    \n",
    "    ## DATE\n",
    "    date_object = soup.find('dd', {'data-automation': 'job-detail-date'})\n",
    "    date = None\n",
    "\n",
    "    try:\n",
    "        date = date_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    date_list.append(date)\n",
    "    \n",
    "    ## LOCATION\n",
    "    location = None\n",
    "    try:\n",
    "        info_header = soup.find('section', {'aria-labelledby': 'jobInfoHeader'})\n",
    "        location_object = info_header.find('strong', {'class': 'lwHBT6d'})\n",
    "        location= location_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    location_list.append(location)\n",
    "    \n",
    "    ## WORK TYPE\n",
    "    work_type = None\n",
    "\n",
    "    try:\n",
    "        work_type_object = soup.find('dd',{'data-automation': 'job-detail-work-type'})\n",
    "        work_type = work_type_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    work_type_list.append(work_type)\n",
    "    \n",
    "    ## JOB TYPE\n",
    "    job_type = None\n",
    "\n",
    "    try:\n",
    "        info_header = soup.find('section', {'aria-labelledby': 'jobInfoHeader'})\n",
    "        job_type = info_header.find_all('dd')[-1].find('strong', {'class': 'lwHBT6d'}).text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    job_type_list.append(job_type)\n",
    "    \n",
    "    ## DESCRIPTION\n",
    "    description = None\n",
    "\n",
    "    try:\n",
    "        description_object = soup.find('div', {'data-automation': 'jobDescription'})\n",
    "        for p in description_object.find_all('p'):\n",
    "            if description == None:\n",
    "                description = p.text\n",
    "            else: \n",
    "                description = description + ' ' + p.text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        description_object = soup.find('div', {'data-automation': 'jobDescription'})\n",
    "        for li in description_object.find_all('li'):\n",
    "            if description == None:\n",
    "                description = li.text\n",
    "            else: \n",
    "                description = description + ' ' + li.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    description_list.append(description)\n",
    "    \n",
    "    \n",
    "seek_data_analytic_df = pd.DataFrame({'url': url_list,\n",
    "                               'job_title': title_list,\n",
    "                               'company': company_list,\n",
    "                               'date': date_list,\n",
    "                               'location': location_list,\n",
    "                               'employment_type': work_type_list,\n",
    "                               'field': job_type_list,\n",
    "                               'description': description_list})\n",
    "\n",
    "seek_data_analytic_df.to_csv('./Q2_data_scraping/seek_data_analytic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1379, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seek_data_analytic_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try scraping detailed page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_path = 'https://www.seek.com.au/job/37457759?type=standard'\n",
    "page = requests.get(detailed_path)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TITLE\n",
    "title_object = soup.find('span', {'data-automation': 'job-detail-title'})\n",
    "title = ''\n",
    "try:\n",
    "    title = title_object.text\n",
    "except:\n",
    "    title = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Company type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = ''\n",
    "\n",
    "try:\n",
    "    company_object = soup.find('span', {'data-automation': 'advertiser-name'})\n",
    "    company = company_object.text\n",
    "    \n",
    "except:\n",
    "    company = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_object = soup.find('dd', {'data-automation': 'job-detail-date'})\n",
    "date = ''\n",
    "\n",
    "try:\n",
    "    date = date_object.text\n",
    "except:\n",
    "    date = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = ''\n",
    "try:\n",
    "    info_header = soup.find('section', {'aria-labelledby': 'jobInfoHeader'})\n",
    "    location_object = info_header.find('strong', {'class': 'lwHBT6d'})\n",
    "    location= location_object.text\n",
    "except:\n",
    "    location = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_type = ''\n",
    "\n",
    "try:\n",
    "    work_type_object = soup.find('dd',{'data-automation': 'job-detail-work-type'})\n",
    "    work_type = work_type_object.text\n",
    "except:\n",
    "    work_type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_type_object.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type = ''\n",
    "\n",
    "try:\n",
    "    info_header = soup.find('section', {'aria-labelledby': 'jobInfoHeader'})\n",
    "    job_type = info_header.find_all('dd')[-1].find('strong', {'class': 'lwHBT6d'}).text\n",
    "except:\n",
    "    job_type = None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = ''\n",
    "\n",
    "description_object = soup.find('div', {'data-automation': 'jobDescription'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = None\n",
    "\n",
    "try:\n",
    "    description_object = soup.find('div', {'data-automation': 'jobDescription'})\n",
    "    for p in description_object.find_all('p'):\n",
    "        if description == None:\n",
    "            description = p.text\n",
    "        else: \n",
    "            description = description + ' ' + p.text\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "    \n",
    "try:\n",
    "    description_object = soup.find('div', {'data-automation': 'jobDescription'})\n",
    "    for li in description_object.find_all('li'):\n",
    "        if description == None:\n",
    "            description = li.text\n",
    "        else: \n",
    "            description = description + ' ' + li.text\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "# import\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path80 = []\n",
    "for i in range(1, 1001):\n",
    "    path = 'https://www.seek.com.au/data-jobs/in-All-Australia?page=' + str(i) + '&salaryrange=0-80000&salarytype=annual'\n",
    "    path80.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf3fa9dc9ee4febbcf42bb27a840c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates 194\n"
     ]
    }
   ],
   "source": [
    "urls80 = []\n",
    "dupe_count = 0\n",
    "for path in log_progress(path80):\n",
    "    # load the page\n",
    "    page = requests.get(path)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # grab title url\n",
    "    titles = soup.find_all('a', {'data-automation': 'jobTitle'}) \n",
    "    for title in titles:\n",
    "        url = 'https://www.seek.com.au' + title.get('href')\n",
    "        if url not in urls80:\n",
    "            urls80.append(url)\n",
    "        else:\n",
    "            dupe_count+=1\n",
    "\n",
    "print('duplicates', dupe_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls80_df = pd.DataFrame(urls80, columns = ['urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls80_df.to_csv('seek_urls_80k.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape all 80k jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls80_df = pd.read_csv('seek_urls_80k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls80 = urls80_df.urls.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd66dfda1a246438cab3f3e6b61daff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=3858)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "title_list = []\n",
    "company_list = []\n",
    "date_list = []\n",
    "location_list = []\n",
    "work_type_list = []\n",
    "job_type_list = []\n",
    "description_list = []\n",
    "url_list = []\n",
    "\n",
    "for url in log_progress(urls80):\n",
    "    detailed_path = url\n",
    "    page = requests.get(detailed_path)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    ## URL\n",
    "    url_list.append(url)\n",
    "    \n",
    "    ## TITLES\n",
    "    title_object = soup.find('span', {'data-automation': 'job-detail-title'})\n",
    "    title = None\n",
    "    try:\n",
    "        title = title_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    title_list.append(title)\n",
    "    \n",
    "    ## COMPANY\n",
    "    company = None\n",
    "\n",
    "    try:\n",
    "        company_object = soup.find('span', {'data-automation': 'advertiser-name'})\n",
    "        company = company_object.text\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    company_list.append(company)\n",
    "    \n",
    "    \n",
    "    ## DATE\n",
    "    date_object = soup.find('dd', {'data-automation': 'job-detail-date'})\n",
    "    date = None\n",
    "\n",
    "    try:\n",
    "        date = date_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    date_list.append(date)\n",
    "    \n",
    "    ## LOCATION\n",
    "    location = None\n",
    "    try:\n",
    "        info_header = soup.find('section', {'aria-labelledby': 'jobInfoHeader'})\n",
    "        location_object = info_header.find('strong', {'class': 'lwHBT6d'})\n",
    "        location= location_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    location_list.append(location)\n",
    "    \n",
    "    ## WORK TYPE\n",
    "    work_type = None\n",
    "\n",
    "    try:\n",
    "        work_type_object = soup.find('dd',{'data-automation': 'job-detail-work-type'})\n",
    "        work_type = work_type_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    work_type_list.append(work_type)\n",
    "    \n",
    "    ## JOB TYPE\n",
    "    job_type = None\n",
    "\n",
    "    try:\n",
    "        info_header = soup.find('section', {'aria-labelledby': 'jobInfoHeader'})\n",
    "        job_type = info_header.find_all('dd')[-1].find('strong', {'class': 'lwHBT6d'}).text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    job_type_list.append(job_type)\n",
    "    \n",
    "    ## DESCRIPTION\n",
    "    description = None\n",
    "\n",
    "    try:\n",
    "        description_object = soup.find('div', {'data-automation': 'jobDescription'})\n",
    "        for p in description_object.find_all('p'):\n",
    "            if description == None:\n",
    "                description = p.text\n",
    "            else: \n",
    "                description = description + ' ' + p.text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        description_object = soup.find('div', {'data-automation': 'jobDescription'})\n",
    "        for li in description_object.find_all('li'):\n",
    "            if description == None:\n",
    "                description = li.text\n",
    "            else: \n",
    "                description = description + ' ' + li.textb\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    description_list.append(description)\n",
    "    \n",
    "\n",
    "seek80k_jobs_df = pd.DataFrame({'url': url_list,\n",
    "                               'job_title': title_list,\n",
    "                               'company': company_list,\n",
    "                               'date': date_list,\n",
    "                               'location': location_list,\n",
    "                               'employment_type': work_type_list,\n",
    "                               'field': job_type_list,\n",
    "                               'description': description_list})\n",
    "\n",
    "seek80k_jobs_df.to_csv('seek80k_jobs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>field</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.seek.com.au/job/37428435?type=prom...</td>\n",
       "      <td>Tender / Bid Leader - Data Collection &amp; Road A...</td>\n",
       "      <td>Australian Road Research Board</td>\n",
       "      <td>10 Oct 2018</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>About us:Established in 1960, the Australian R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.seek.com.au/job/37512965?type=prom...</td>\n",
       "      <td>Commercial Analyst</td>\n",
       "      <td>None</td>\n",
       "      <td>19 Oct 2018</td>\n",
       "      <td>Cairns &amp; Far North</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>Coral Expeditions is Australia's Pioneering Ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.seek.com.au/job/37523713?type=stan...</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Youfoodz Pty Ltd</td>\n",
       "      <td>22 Oct 2018</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Marketing &amp; Communications</td>\n",
       "      <td>You've heard about us…You've watched as we hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.seek.com.au/job/37523959?type=stan...</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Polyglot Group</td>\n",
       "      <td>22 Oct 2018</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Mining, Resources &amp; Energy</td>\n",
       "      <td>Our client is an Australian company providing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.seek.com.au/job/37522935?type=stan...</td>\n",
       "      <td>Data Administrator</td>\n",
       "      <td>None</td>\n",
       "      <td>22 Oct 2018</td>\n",
       "      <td>ACT</td>\n",
       "      <td>Part Time</td>\n",
       "      <td>Education &amp; Training</td>\n",
       "      <td>Classification: ANU Officer 4 (IT)Salary packa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.seek.com.au/job/37428435?type=prom...   \n",
       "1  https://www.seek.com.au/job/37512965?type=prom...   \n",
       "2  https://www.seek.com.au/job/37523713?type=stan...   \n",
       "3  https://www.seek.com.au/job/37523959?type=stan...   \n",
       "4  https://www.seek.com.au/job/37522935?type=stan...   \n",
       "\n",
       "                                           job_title  \\\n",
       "0  Tender / Bid Leader - Data Collection & Road A...   \n",
       "1                                 Commercial Analyst   \n",
       "2                                       Data Analyst   \n",
       "3                                       Data Analyst   \n",
       "4                                 Data Administrator   \n",
       "\n",
       "                          company         date            location  \\\n",
       "0  Australian Road Research Board  10 Oct 2018            Brisbane   \n",
       "1                            None  19 Oct 2018  Cairns & Far North   \n",
       "2                Youfoodz Pty Ltd  22 Oct 2018            Brisbane   \n",
       "3                  Polyglot Group  22 Oct 2018           Melbourne   \n",
       "4                            None  22 Oct 2018                 ACT   \n",
       "\n",
       "  employment_type                       field  \\\n",
       "0       Full Time                 Engineering   \n",
       "1       Full Time                  Accounting   \n",
       "2       Full Time  Marketing & Communications   \n",
       "3       Full Time  Mining, Resources & Energy   \n",
       "4       Part Time        Education & Training   \n",
       "\n",
       "                                         description  \n",
       "0  About us:Established in 1960, the Australian R...  \n",
       "1  Coral Expeditions is Australia's Pioneering Ex...  \n",
       "2  You've heard about us…You've watched as we hav...  \n",
       "3  Our client is an Australian company providing ...  \n",
       "4  Classification: ANU Officer 4 (IT)Salary packa...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seek80k_jobs_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100k+ jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path100 = []\n",
    "for i in range(1, 1001):\n",
    "    path = 'https://www.seek.com.au/data-jobs/in-Australia?page=' + str(i) + '&salaryrange=100000-999999&salarytype=annual'\n",
    "    path100.append(path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b250e7a47c004e3e82ee3520682a612f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=1000)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates 197\n"
     ]
    }
   ],
   "source": [
    "urls100 = []\n",
    "dupe_count = 0\n",
    "for path in log_progress(path100):\n",
    "    # load the page\n",
    "    page = requests.get(path)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # grab title url\n",
    "    titles = soup.find_all('a', {'data-automation': 'jobTitle'}) \n",
    "    for title in titles:\n",
    "        url = 'https://www.seek.com.au' + title.get('href')\n",
    "        if url not in urls80 and url not in urls100:\n",
    "            urls100.append(url)\n",
    "        else:\n",
    "            dupe_count+=1\n",
    "\n",
    "print('duplicates', dupe_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls100_df = pd.DataFrame(urls100, columns = ['urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls100_df.to_csv('seek_urls_100k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape all 100k jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls100_df = pd.read_csv('seek_urls_100k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls100 = urls100_df.urls.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52f09e270954c2cb43d74959d357b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=3819)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "title_list = []\n",
    "company_list = []\n",
    "date_list = []\n",
    "location_list = []\n",
    "work_type_list = []\n",
    "job_type_list = []\n",
    "description_list = []\n",
    "url_list = []\n",
    "\n",
    "for url in log_progress(urls100):\n",
    "    detailed_path = url\n",
    "    page = requests.get(detailed_path)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    ## URL\n",
    "    url_list.append(url)\n",
    "    \n",
    "    ## TITLES\n",
    "    title_object = soup.find('span', {'data-automation': 'job-detail-title'})\n",
    "    title = None\n",
    "    try:\n",
    "        title = title_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    title_list.append(title)\n",
    "    \n",
    "    ## COMPANY\n",
    "    company = None\n",
    "\n",
    "    try:\n",
    "        company_object = soup.find('span', {'data-automation': 'advertiser-name'})\n",
    "        company = company_object.text\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    company_list.append(company)\n",
    "    \n",
    "    \n",
    "    ## DATE\n",
    "    date_object = soup.find('dd', {'data-automation': 'job-detail-date'})\n",
    "    date = None\n",
    "\n",
    "    try:\n",
    "        date = date_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    date_list.append(date)\n",
    "    \n",
    "    ## LOCATION\n",
    "    location = None\n",
    "    try:\n",
    "        info_header = soup.find('section', {'aria-labelledby': 'jobInfoHeader'})\n",
    "        location_object = info_header.find('strong', {'class': 'lwHBT6d'})\n",
    "        location= location_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    location_list.append(location)\n",
    "    \n",
    "    ## WORK TYPE\n",
    "    work_type = None\n",
    "\n",
    "    try:\n",
    "        work_type_object = soup.find('dd',{'data-automation': 'job-detail-work-type'})\n",
    "        work_type = work_type_object.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    work_type_list.append(work_type)\n",
    "    \n",
    "    ## JOB TYPE\n",
    "    job_type = None\n",
    "\n",
    "    try:\n",
    "        info_header = soup.find('section', {'aria-labelledby': 'jobInfoHeader'})\n",
    "        job_type = info_header.find_all('dd')[-1].find('strong', {'class': 'lwHBT6d'}).text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    job_type_list.append(job_type)\n",
    "    \n",
    "    ## DESCRIPTION\n",
    "    description = None\n",
    "\n",
    "    try:\n",
    "        description_object = soup.find('div', {'data-automation': 'jobDescription'})\n",
    "        for p in description_object.find_all('p'):\n",
    "            if description == None:\n",
    "                description = p.text\n",
    "            else: \n",
    "                description = description + ' ' + p.text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        description_object = soup.find('div', {'data-automation': 'jobDescription'})\n",
    "        for li in description_object.find_all('li'):\n",
    "            if description == None:\n",
    "                description = li.text\n",
    "            else: \n",
    "                description = description + ' ' + li.text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    description_list.append(description)\n",
    "    \n",
    "    \n",
    "seek100k_jobs_df = pd.DataFrame({'url': url_list,\n",
    "                               'job_title': title_list,\n",
    "                               'company': company_list,\n",
    "                               'date': date_list,\n",
    "                               'location': location_list,\n",
    "                               'employment_type': work_type_list,\n",
    "                               'field': job_type_list,\n",
    "                               'description': description_list})\n",
    "\n",
    "seek100k_jobs_df.to_csv('seek100k_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seek100k_jobs_df.field.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try scraping detailed page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_path = 'https://www.seek.com.au/job/37457759?type=standard'\n",
    "page = requests.get(detailed_path)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TITLE\n",
    "title_object = soup.find('span', {'data-automation': 'job-detail-title'})\n",
    "title = ''\n",
    "try:\n",
    "    title = title_object.text\n",
    "except:\n",
    "    title = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Company type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = ''\n",
    "\n",
    "try:\n",
    "    company_object = soup.find('span', {'data-automation': 'advertiser-name'})\n",
    "    company = company_object.text\n",
    "    \n",
    "except:\n",
    "    company = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_object = soup.find('dd', {'data-automation': 'job-detail-date'})\n",
    "date = ''\n",
    "\n",
    "try:\n",
    "    date = date_object.text\n",
    "except:\n",
    "    date = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = ''\n",
    "try:\n",
    "    info_header = soup.find('section', {'aria-labelledby': 'jobInfoHeader'})\n",
    "    location_object = info_header.find('strong', {'class': 'lwHBT6d'})\n",
    "    location= location_object.text\n",
    "except:\n",
    "    location = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_type = ''\n",
    "\n",
    "try:\n",
    "    work_type_object = soup.find('dd',{'data-automation': 'job-detail-work-type'})\n",
    "    work_type = work_type_object.text\n",
    "except:\n",
    "    work_type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_type_object.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type = ''\n",
    "\n",
    "try:\n",
    "    info_header = soup.find('section', {'aria-labelledby': 'jobInfoHeader'})\n",
    "    job_type = info_header.find_all('dd')[-1].find('strong', {'class': 'lwHBT6d'}).text\n",
    "except:\n",
    "    job_type = None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = ''\n",
    "\n",
    "description_object = soup.find('div', {'data-automation': 'jobDescription'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = None\n",
    "\n",
    "try:\n",
    "    description_object = soup.find('div', {'data-automation': 'jobDescription'})\n",
    "    for p in description_object.find_all('p'):\n",
    "        if description == None:\n",
    "            description = p.text\n",
    "        else: \n",
    "            description = description + ' ' + p.text\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "    \n",
    "try:\n",
    "    description_object = soup.find('div', {'data-automation': 'jobDescription'})\n",
    "    for li in description_object.find_all('li'):\n",
    "        if description == None:\n",
    "            description = li.text\n",
    "        else: \n",
    "            description = description + ' ' + li.text\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
